{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets are loaded. Total # of tweets: 7613.\n",
      "# of labels:\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_csv_path = 'train.csv'\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "all_texts = train_df['text'].values.tolist()\n",
    "all_labels = train_df['target'].values.tolist()\n",
    "\n",
    "print(\"Tweets are loaded. Total # of tweets: {}.\".format(len(all_texts)))\n",
    "print(\"# of labels:\")\n",
    "print(train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweeets which appear multiple times: 19\n",
      "Tweets which have inconsistent labeling:\n",
      "\n",
      "To fight bioterrorism sir.\n",
      "[1, 0, 1, 0]\n",
      ".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\n",
      "[1, 1, 0, 1]\n",
      "He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\n",
      "[0, 1, 1, 0, 0, 0]\n",
      "Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\n",
      " \n",
      "#FARRAKHAN #QUOTE\n",
      "[1, 0, 0]\n",
      "#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\n",
      "[1, 1, 0]\n",
      "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\n",
      "[0, 0, 1, 0, 0, 1]\n",
      "Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife\n",
      "[0, 1, 0]\n",
      "#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\n",
      "[0, 0, 1]\n",
      "CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\n",
      "[1, 1, 0]\n",
      "that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\n",
      "[1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "frequent_tweets = {}\n",
    "for t, l in zip(all_texts, all_labels):\n",
    "    if all_texts.count(t) > 2:\n",
    "        frequent_tweets[t] = [l] if t not in frequent_tweets else frequent_tweets[t] + [l]\n",
    "        \n",
    "print(\"The number of tweeets which appear multiple times: {}\"\n",
    "      .format(len(frequent_tweets.keys())))     \n",
    "\n",
    "print(\"Tweets which have inconsistent labeling:\")\n",
    "print()\n",
    "\n",
    "for t, ls in frequent_tweets.items():\n",
    "    if not all(element == ls[0] for element in ls):\n",
    "        print(t)\n",
    "        print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relabeled 10 tweets in total\n"
     ]
    }
   ],
   "source": [
    "should_be_real = [\".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\",\n",
    "                 \"#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\",\n",
    "                 \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\"]\n",
    "\n",
    "should_not_be_real = [\"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\",\n",
    "                     \"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\",\n",
    "                      \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
    "                     \"Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife\",\n",
    "                     \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\",\n",
    "                     \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\",\n",
    "                     \"To fight bioterrorism sir.\"]\n",
    "def fix_labels(tweets_to_fix, correct_label):\n",
    "    for i, (tweet, label) in enumerate(zip(all_texts, all_labels)):\n",
    "        if any(tweet.startswith(t) for t in tweets_to_fix):\n",
    "            all_labels[i] = correct_label\n",
    "\n",
    "        \n",
    "fix_labels(should_be_real, 1)\n",
    "fix_labels(should_not_be_real, 0)\n",
    "\n",
    "print(\"Relabeled {} tweets in total\".format(len(should_be_real) + len(should_not_be_real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data is read and split into training and validation sets.\n",
      "Size of train data (# of entries): 5709\n",
      "Size of validation data (# of entries): 1904\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    all_texts, all_labels,\n",
    "    stratify = train_df['target']\n",
    ")\n",
    "\n",
    "print('Train data is read and split into training and validation sets.')\n",
    "print('Size of train data (# of entries): {}'.format(len(train_texts)))\n",
    "print('Size of validation data (# of entries): {}'.format(len(val_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def remove_urls(tweet):\n",
    "    return re.sub(r\"http(s?)://[\\S]+\", '', tweet)\n",
    "\n",
    "def remove_at_links(tweet):\n",
    "    return re.sub(r\"\\B(@)\\S+\", '', tweet)\n",
    "\n",
    "def remove_non_ascii_chars(tweet):\n",
    "    ascii_chars = set(string.printable)\n",
    "    for c in tweet:\n",
    "        if c not in ascii_chars:\n",
    "            tweet = tweet.replace(c,'')\n",
    "    return tweet\n",
    "def fix_ax_nots(tweet):\n",
    "    tweet = tweet.replace(\" dont \", \" do not \")\n",
    "    tweet = tweet.replace(\" don't \", \" do not \")\n",
    "    tweet = tweet.replace(\" doesnt \", \" does not \")\n",
    "    tweet = tweet.replace(\" doesn't \", \" does not \")\n",
    "    tweet = tweet.replace(\" wont \", \" will not \")\n",
    "    tweet = tweet.replace(\" won't \", \" will not \")\n",
    "    tweet = tweet.replace(\" cant \", \" cannot \")\n",
    "    tweet = tweet.replace(\" can't \", \" cannot \")\n",
    "    tweet = tweet.replace(\" couldnt \", \" could not \")\n",
    "    tweet = tweet.replace(\" couldn't \", \" could not \")\n",
    "    tweet = tweet.replace(\" shouldnt \", \" should not \")\n",
    "    tweet = tweet.replace(\" shouldn't \", \" should not \")\n",
    "    tweet = tweet.replace(\" wouldnt \", \" would not \")\n",
    "    tweet = tweet.replace(\" wouldn't \", \" would not \")\n",
    "    tweet = tweet.replace(\" mustnt \", \" must not \")\n",
    "    tweet = tweet.replace(\" mustn't \", \" must not \")\n",
    "    \n",
    "    return tweet\n",
    "def fix_personal_pronouns_and_verb(tweet):\n",
    "    tweet = tweet.replace(\" im \", \" i am \")\n",
    "    tweet = tweet.replace(\" youre \", \" you are\")\n",
    "    tweet = tweet.replace(\" hes \", \" he is\") # ? he's can be he has as well\n",
    "    tweet = tweet.replace(\" shes \", \" she is\")\n",
    "    # we are -> we're -> were  ---- were is a valid word\n",
    "    tweet = tweet.replace(\" theyre \", \" they are\")\n",
    "    \n",
    "    tweet = tweet.replace(\" ive \", \" i have \")\n",
    "    tweet = tweet.replace(\" youve \", \" you have \")\n",
    "    tweet = tweet.replace(\" weve \", \" we have \")\n",
    "    tweet = tweet.replace(\" theyve \", \" they have \")\n",
    "    \n",
    "    tweet = tweet.replace(\" youll \", \" you will \")\n",
    "    tweet = tweet.replace(\" theyll \", \" they will \")\n",
    "    \n",
    "    return tweet\n",
    "def fix_special_chars(tweet):\n",
    "    tweet = tweet.replace(\"&amp;\", \" and \")\n",
    "    # tweet = tweet.replace(\"--&gt;\", \"\")\n",
    "    return tweet\n",
    "        \n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = remove_urls(tweet)\n",
    "    tweet = remove_at_links(tweet)\n",
    "    tweet = remove_non_ascii_chars(tweet)\n",
    "    tweet = fix_special_chars(tweet)\n",
    "    tweet = fix_ax_nots(tweet)\n",
    "    tweet = fix_personal_pronouns_and_verb(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tweets cleaned.\n",
      "Validation tweets cleaned.\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_texts = [clean_tweet(tweet) for tweet in train_texts]\n",
    "print(\"Train tweets cleaned.\")\n",
    "cleaned_val_texts = [clean_tweet(tweet) for tweet in val_texts]\n",
    "print(\"Validation tweets cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6a906240ea43dc8e9c85b6009170e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Softwares\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dell\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a02eab7f5d84a3cb65405defffffd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175abb58a3c84db7b8e4bb0c711f9172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab18e40284b14aa5b1819071cece6fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & validation texts encoded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(cleaned_train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(cleaned_val_texts, truncation=True, padding=True)\n",
    "\n",
    "print('Train & validation texts encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Class to store the tweet data as PyTorch Dataset\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Class to store the tweet data as PyTorch Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # an encoding can have keys such as input_ids and attention_mask\n",
    "        # item is a dictionary which has the same keys as the encoding has\n",
    "        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n",
    "print(TweetDataset.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu.\n"
     ]
    }
   ],
   "source": [
    "# device (turn on GPU acceleration for faster execution)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device used: {}.\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A pre-trained BERT model with a custom classifier.\n",
      "    The classifier is a neural network implemented in this class.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "in_features = 768 # it's 768 because that's the size of the output provided by the underlying BERT model\n",
    "class BertWithCustomNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A pre-trained BERT model with a custom classifier.\n",
    "    The classifier is a neural network implemented in this class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, linear_size):\n",
    "        super(BertWithCustomNNClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.linear1 = nn.Linear(in_features=in_features, out_features=linear_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features=linear_size)\n",
    "        self.dropout2 = nn.Dropout(p=0.8)\n",
    "        self.linear2 = nn.Linear(in_features=linear_size, out_features=1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, tokens, attention_mask):\n",
    "        bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n",
    "        x = self.dropout1(bert_output[1])\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        return self.sigmoid(x)\n",
    "        \n",
    "    def freeze_bert(self):\n",
    "        \"\"\"\n",
    "        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        only the wieghts of the custom classifier are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.named_parameters():\n",
    "            param[1].requires_grad=False\n",
    "\n",
    "    def unfreeze_bert(self):\n",
    "        \"\"\"\n",
    "        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        both the wieghts of the custom classifier and of the underlying BERT are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.named_parameters():\n",
    "            param[1].requires_grad=True\n",
    "\n",
    "            \n",
    "print(BertWithCustomNNClassifier.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return batches of accuracy and f1 scores.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def eval_prediction(y_batch_actual, y_batch_predicted):\n",
    "    \"\"\"Return batches of accuracy and f1 scores.\"\"\"\n",
    "    y_batch_actual_np = y_batch_actual.cpu().detach().numpy()\n",
    "    y_batch_predicted_np = np.round(y_batch_predicted.cpu().detach().numpy())\n",
    "    \n",
    "    acc = accuracy_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np)\n",
    "    f1 = f1_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np, average='weighted')\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "print(eval_prediction.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1\n",
      "Learning rate: 0.000027\n",
      "Batch size: 16\n",
      "The number of hidden layers in the custom head: 8\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "num_of_epochs = 1\n",
    "learning_rate = 27e-6\n",
    "batch_size = 16\n",
    "hidden_layers = 8\n",
    "\n",
    "print(\"Epochs: {}\".format(num_of_epochs))\n",
    "print(\"Learning rate: {:.6f}\".format(learning_rate))\n",
    "print(\"Batch size: {}\".format(batch_size))\n",
    "print(\"The number of hidden layers in the custom head: {}\".format(hidden_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d956cae0e18a47f9a798fc755b149c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertWithCustomNNClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (linear1): Linear(in_features=768, out_features=8, bias=True)\n",
       "  (batch_norm1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.8, inplace=False)\n",
       "  (linear2): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (batch_norm2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertWithCustomNNClassifier(linear_size=hidden_layers)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "print('Initialized optimizer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized loss function.\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "print('Initialized loss function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train & val datasets.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataset & dataloader\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Created train & val datasets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method to train the model\n"
     ]
    }
   ],
   "source": [
    "def training_step(dataloader, model, optimizer, loss_fn, if_freeze_bert):\n",
    "    \"\"\"Method to train the model\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n",
    "      \n",
    "    epoch_loss = 0\n",
    "    size = len(dataloader.dataset)\n",
    " \n",
    "    for i, batch in enumerate(dataloader):        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        outputs = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n",
    "                        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            \n",
    "print(training_step.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method to test the model's accuracy and loss on the validation set\n"
     ]
    }
   ],
   "source": [
    "def validation_step(dataloader, model, loss_fn):\n",
    "    \"\"\"Method to test the model's accuracy and loss on the validation set\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    model.freeze_bert()\n",
    "    \n",
    "    size = len(dataloader)\n",
    "    f1, acc = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            y = batch['labels'].to(device)\n",
    "                  \n",
    "            pred = model(tokens=X, attention_mask=attention_mask)\n",
    "            \n",
    "            acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n",
    "            acc += acc_batch\n",
    "            f1 += f1_batch\n",
    "\n",
    "        acc = acc/size\n",
    "        f1 = f1/size\n",
    "                \n",
    "    return acc, f1\n",
    "print(validation_step.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2129cad33a0a44598b7be64cd5d132b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: #1\n",
      "Bert is not freezed\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "best_acc, best_f1 = 0, 0\n",
    "path = './best_model.pt'\n",
    "if_freeze_bert = False\n",
    "\n",
    "for i in tqdm(range(num_of_epochs)):\n",
    "    print(\"Epoch: #{}\".format(i+1))\n",
    "\n",
    "    if i < 5:\n",
    "        if_freeze_bert = False\n",
    "        print(\"Bert is not freezed\")\n",
    "    else:\n",
    "        if_freeze_bert = True\n",
    "        print(\"Bert is freezed\")\n",
    "    \n",
    "    training_step(train_loader, model,optimizer, loss_fn, if_freeze_bert)\n",
    "    train_acc, train_f1 = validation_step(train_loader, model, loss_fn)\n",
    "    val_acc, val_f1 = validation_step(val_loader, model, loss_fn)\n",
    "    \n",
    "    print(\"Training results: \")\n",
    "    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n",
    "\n",
    "    print(\"Validation results: \")\n",
    "    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc    \n",
    "        torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "\n",
    "clean_test_texts = [clean_tweet(tweet) for tweet in test_data['text'].values.tolist()]\n",
    "test_encodings = tokenizer(clean_test_texts,\n",
    "                           truncation=True, padding=True,\n",
    "                           return_tensors='pt').to(device)\n",
    "\n",
    "print(\"Encodings are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(path)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens=test_encodings['input_ids'], attention_mask=test_encodings['attention_mask'])\n",
    "    \n",
    "binary_predictions = np.round(predictions.cpu().detach().numpy()).astype(int).flatten()\n",
    "    \n",
    "print(\"Predictions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target'] = binary_predictions\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "print('Predictions are saved to submission.csv.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
